{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ayrat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easydict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ec260003ee97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_loader_example\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconfig_example\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTabNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mValidation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\DataEducation\\project\\tabnet\\config_example.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0measydict\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEasyDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEasyDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'easydict'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_DUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from datetime import datetime\n",
    "\n",
    "from data_loader_example import DataLoader\n",
    "from config_example import cfg\n",
    "from network import TabNet\n",
    "from validation import Validation\n",
    "from plots import Plots\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "class Inflow:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_loader = DataLoader(cfg)\n",
    "        self.__build_model()\n",
    "\n",
    "        self.log_config('config', cfg, self.train_summary_writer, 0)\n",
    "\n",
    "        self.validation = Validation(cfg.validation, self.data_loader, self.valid_summary_writer, self.network, 'Valid')\n",
    "\n",
    "    def log_config(self, name, config, summary_writer, step):\n",
    "        general_keys = list(config.keys())\n",
    "        rows = []\n",
    "        for key in general_keys:\n",
    "            try:\n",
    "                subkeys = list(config[key])\n",
    "                for subkey in subkeys:\n",
    "                    rows.append(['%s.%s' % (key, subkey), str(config[key][subkey])])\n",
    "            except:\n",
    "                rows.append([key, str(config[key])])\n",
    "\n",
    "        hyperparameters = [tf.convert_to_tensor(row) for row in rows]\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.text(name, tf.stack(hyperparameters), step=step)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __build_model(self):\n",
    "        self.network = TabNet(self.data_loader.get_columns(),\n",
    "                             self.data_loader.num_features,\n",
    "                             feature_dim=cfg.train.feature_dim,\n",
    "                             output_dim=cfg.train.output_dim,\n",
    "                             num_decision_steps=cfg.train.num_decision_steps,\n",
    "                             relaxation_factor=cfg.train.relaxation_factor,\n",
    "                             batch_momentum=cfg.train.batch_momentum,\n",
    "                             virtual_batch_size=cfg.train.virtual_batch_size,\n",
    "                             num_classes=cfg.train.num_classes,\n",
    "                             encoder_type=cfg.train.encoder_type,\n",
    "                             epsilon=0.00001)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            cfg.train.learning_rate,\n",
    "            decay_steps=cfg.train.learning_rate_decay_steps,\n",
    "            decay_rate=cfg.train.learning_rate_decay_factor,\n",
    "            staircase=True\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.log_loss = tf.losses.BinaryCrossentropy()\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.AUC_metric = tf.keras.metrics.AUC(name='train_AUC')\n",
    "        self.binary_metric = tf.keras.metrics.BinaryAccuracy(name='train_BinaryAccuracy')\n",
    "\n",
    "        train_log_dir = os.path.join(cfg.train.logs_base_dir, 'train')\n",
    "        valid_log_dir = os.path.join(cfg.train.logs_base_dir, 'valid')\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "        self.valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)\n",
    "        return\n",
    "\n",
    "    def __reset_all_metrics(self):\n",
    "        self.loss_metric.reset_states()\n",
    "        self.AUC_metric.reset_states()\n",
    "        self.binary_metric.reset_states()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __load_variables(self):\n",
    "        if cfg.train.restore_model_path != '':\n",
    "            print(cfg.train.restore_model_path)\n",
    "            output = self.network(dict(self.data_loader.valid_X.take([0, 1], axis=0)), training=False)\n",
    "            \n",
    "            file = h5py.File(cfg.train.restore_model_path, 'r')\n",
    "            weights = []\n",
    "            for i in range(len(file.keys())):\n",
    "                weights.append(file['weight' + str(i)].value)\n",
    "            self.network.set_weights(weights)\n",
    "            file.close()\n",
    "\n",
    "#             file = h5py.File(cfg.train.restore_model_path.replace('model-', 'optimizer-'), 'r')\n",
    "#             weights = []\n",
    "#             for i in range(len(file.keys())):\n",
    "#                 weights.append(file['weight' + str(i)].value)\n",
    "#             self.optimizer.set_weights(weights)\n",
    "#             file.close()\n",
    "\n",
    "            step = int(cfg.train.restore_model_path.split('.h5')[0].split('-')[-1])\n",
    "            self.optimizer.iterations.assign(step)\n",
    "\n",
    "            print('Model restored')\n",
    "        else:\n",
    "            step = 0\n",
    "\n",
    "        return step\n",
    "\n",
    "    def __save_model(self, step):\n",
    "        if not os.path.exists(cfg.train.models_base_dir):\n",
    "            os.makedirs(cfg.train.models_base_dir)\n",
    "\n",
    "        file_path = os.path.join(cfg.train.models_base_dir, 'model-%d.h5' % step)\n",
    "        file = h5py.File(file_path, 'w')\n",
    "        weights = self.network.get_weights()\n",
    "        for i in range(len(weights)):\n",
    "            file.create_dataset('weight' + str(i), data=weights[i])\n",
    "        file.close()\n",
    "\n",
    "        file_path = os.path.join(cfg.train.models_base_dir, 'optimizer-%d.h5' % step)\n",
    "        file = h5py.File(file_path, 'w')\n",
    "        weights = self.optimizer.get_weights()\n",
    "        for i in range(len(weights)):\n",
    "            file.create_dataset('weight' + str(i), data=weights[i])\n",
    "        file.close()\n",
    "\n",
    "        print('Model saved')\n",
    "\n",
    "        return\n",
    "\n",
    "    def run_train(self):\n",
    "        step = self.__load_variables()\n",
    "\n",
    "        self.validation.run_validation(step)\n",
    "\n",
    "        for epoch in range(cfg.train.max_nrof_epochs):\n",
    "            print('Start epoch %d' % epoch)\n",
    "\n",
    "            self.__reset_all_metrics()\n",
    "\n",
    "            batch_id = 0\n",
    "            for batch_features, labels in tqdm(self.data_loader.train_loader):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output, output_aggregated, total_entropy,\\\n",
    "                    aggregated_mask_values_all, mask_values_all = self.network(batch_features, training=True)\n",
    "\n",
    "                    loss = self.log_loss(labels, output)\n",
    "                    # reg_loss = sum(self.network.losses)\n",
    "                    reg_loss = cfg.train.weight_decay * tf.add_n([tf.nn.l2_loss(w) for w in self.network.trainable_variables])\n",
    "                    total_loss = loss + cfg.train.sparsity_loss_weight * total_entropy + reg_loss\n",
    "\n",
    "                grads = tape.gradient(total_loss, self.network.trainable_variables)\n",
    "                capped_gvs = [tf.clip_by_value(grad, -cfg.train.gradient_thresh,\n",
    "                                                cfg.train.gradient_thresh) for grad in grads]\n",
    "\n",
    "                self.optimizer.apply_gradients(zip(capped_gvs, self.network.trainable_variables))\n",
    "\n",
    "                self.loss_metric(total_loss)\n",
    "                self.AUC_metric(labels, output)\n",
    "                self.binary_metric(labels, output)\n",
    "\n",
    "                with self.train_summary_writer.as_default():\n",
    "                    # Visualization of the feature selection mask at decision step ni\n",
    "                    for ni in range(len(mask_values_all)):\n",
    "                        tf.summary.image(\n",
    "                            \"Mask for step\" + str(ni),\n",
    "                            tf.expand_dims(tf.expand_dims(mask_values_all[ni], 0), 3),\n",
    "                            max_outputs=1, step=step)\n",
    "                    # Visualization of the aggregated feature importances\n",
    "                    for ni in range(len(aggregated_mask_values_all)):\n",
    "                        tf.summary.image(\n",
    "                            \"Aggregated mask\",\n",
    "                            tf.expand_dims(tf.expand_dims(aggregated_mask_values_all[ni], 0), 3),\n",
    "                            max_outputs=1, step=step)\n",
    "\n",
    "                    tf.summary.scalar('Total_loss', self.loss_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Total entropy', total_entropy, step=step)\n",
    "                    tf.summary.scalar('Log_Loss', loss, step=step)\n",
    "                    tf.summary.scalar('Reg_Loss', reg_loss, step=step)\n",
    "                    tf.summary.scalar('AUC_score', self.AUC_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Accuracy', self.binary_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Gini_index', 2 * self.AUC_metric.result() - 1, step=step)\n",
    "                    tf.summary.scalar('Learning_rate', self.optimizer.learning_rate.__call__(step).numpy(), step=step)\n",
    "\n",
    "                template = 'Epoch [{}][{}/{}], loss: {}, binary accuracy: {}, AUC: {}'\n",
    "                print(template.format(epoch, batch_id, self.data_loader.nrof_batches,\n",
    "                                      loss, self.binary_metric.result() * 100,\n",
    "                                      self.AUC_metric.result() * 100))\n",
    "\n",
    "                batch_id += 1\n",
    "                step += 1\n",
    "            \n",
    "            self.__save_model(step)\n",
    "            \n",
    "            self.validation.run_validation(step)\n",
    "            \n",
    "            self.data_loader.shuffle_data()\n",
    "        \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data loading ...\n"
     ]
    }
   ],
   "source": [
    "# top25 with co feats by Gini only 4 DS\n",
    "inflow = Inflow()\n",
    "inflow.run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
