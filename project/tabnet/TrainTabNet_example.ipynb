{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (network.py, line 125)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3331\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-ec260003ee97>\"\u001b[1;36m, line \u001b[1;32m18\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from network import TabNet\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Admin\\Documents\\Firyuza\\TabNet\\network.py\"\u001b[1;36m, line \u001b[1;32m125\u001b[0m\n\u001b[1;33m    output = self.encoder_output(output_aggregated)\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_DUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from datetime import datetime\n",
    "\n",
    "from data_loader_example import DataLoader\n",
    "from config_example import cfg\n",
    "from network import TabNet\n",
    "from validation import Validation\n",
    "from plots import Plots\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "class Inflow:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_loader = DataLoader(cfg)\n",
    "        self.__build_model()\n",
    "\n",
    "        self.log_config('config', cfg, self.train_summary_writer, 0)\n",
    "\n",
    "        self.validation = Validation(cfg.validation, self.data_loader, self.valid_summary_writer, self.network, 'Valid')\n",
    "\n",
    "    def log_config(self, name, config, summary_writer, step):\n",
    "        general_keys = list(config.keys())\n",
    "        rows = []\n",
    "        for key in general_keys:\n",
    "            try:\n",
    "                subkeys = list(config[key])\n",
    "                for subkey in subkeys:\n",
    "                    rows.append(['%s.%s' % (key, subkey), str(config[key][subkey])])\n",
    "            except:\n",
    "                rows.append([key, str(config[key])])\n",
    "\n",
    "        hyperparameters = [tf.convert_to_tensor(row) for row in rows]\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.text(name, tf.stack(hyperparameters), step=step)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __build_model(self):\n",
    "        self.network = TabNet(self.data_loader.get_columns(),\n",
    "                             self.data_loader.num_features,\n",
    "                             feature_dim=cfg.train.feature_dim,\n",
    "                             output_dim=cfg.train.output_dim,\n",
    "                             num_decision_steps=cfg.train.num_decision_steps,\n",
    "                             relaxation_factor=cfg.train.relaxation_factor,\n",
    "                             batch_momentum=cfg.train.batch_momentum,\n",
    "                             virtual_batch_size=cfg.train.virtual_batch_size,\n",
    "                             num_classes=cfg.train.num_classes,\n",
    "                             encoder_type=cfg.train.encoder_type,\n",
    "                             epsilon=0.00001)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            cfg.train.learning_rate,\n",
    "            decay_steps=cfg.train.learning_rate_decay_steps,\n",
    "            decay_rate=cfg.train.learning_rate_decay_factor,\n",
    "            staircase=True\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.log_loss = tf.losses.BinaryCrossentropy()\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.AUC_metric = tf.keras.metrics.AUC(name='train_AUC')\n",
    "        self.binary_metric = tf.keras.metrics.BinaryAccuracy(name='train_BinaryAccuracy')\n",
    "\n",
    "        train_log_dir = os.path.join(cfg.train.logs_base_dir, 'train')\n",
    "        valid_log_dir = os.path.join(cfg.train.logs_base_dir, 'valid')\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "        self.valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)\n",
    "        return\n",
    "\n",
    "    def __reset_all_metrics(self):\n",
    "        self.loss_metric.reset_states()\n",
    "        self.AUC_metric.reset_states()\n",
    "        self.binary_metric.reset_states()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __load_variables(self):\n",
    "        if cfg.train.restore_model_path != '':\n",
    "            print(cfg.train.restore_model_path)\n",
    "            output = self.network(dict(self.data_loader.valid_X.take([0, 1], axis=0)), training=False)\n",
    "            \n",
    "            file = h5py.File(cfg.train.restore_model_path, 'r')\n",
    "            weights = []\n",
    "            for i in range(len(file.keys())):\n",
    "                weights.append(file['weight' + str(i)].value)\n",
    "            self.network.set_weights(weights)\n",
    "            file.close()\n",
    "\n",
    "#             file = h5py.File(cfg.train.restore_model_path.replace('model-', 'optimizer-'), 'r')\n",
    "#             weights = []\n",
    "#             for i in range(len(file.keys())):\n",
    "#                 weights.append(file['weight' + str(i)].value)\n",
    "#             self.optimizer.set_weights(weights)\n",
    "#             file.close()\n",
    "\n",
    "            step = int(cfg.train.restore_model_path.split('.h5')[0].split('-')[-1])\n",
    "            self.optimizer.iterations.assign(step)\n",
    "\n",
    "            print('Model restored')\n",
    "        else:\n",
    "            step = 0\n",
    "\n",
    "        return step\n",
    "\n",
    "    def __save_model(self, step):\n",
    "        if not os.path.exists(cfg.train.models_base_dir):\n",
    "            os.makedirs(cfg.train.models_base_dir)\n",
    "\n",
    "        file_path = os.path.join(cfg.train.models_base_dir, 'model-%d.h5' % step)\n",
    "        file = h5py.File(file_path, 'w')\n",
    "        weights = self.network.get_weights()\n",
    "        for i in range(len(weights)):\n",
    "            file.create_dataset('weight' + str(i), data=weights[i])\n",
    "        file.close()\n",
    "\n",
    "        file_path = os.path.join(cfg.train.models_base_dir, 'optimizer-%d.h5' % step)\n",
    "        file = h5py.File(file_path, 'w')\n",
    "        weights = self.optimizer.get_weights()\n",
    "        for i in range(len(weights)):\n",
    "            file.create_dataset('weight' + str(i), data=weights[i])\n",
    "        file.close()\n",
    "\n",
    "        print('Model saved')\n",
    "\n",
    "        return\n",
    "\n",
    "    def run_train(self):\n",
    "        step = self.__load_variables()\n",
    "\n",
    "        self.validation.run_validation(step)\n",
    "\n",
    "        for epoch in range(cfg.train.max_nrof_epochs):\n",
    "            print('Start epoch %d' % epoch)\n",
    "\n",
    "            self.__reset_all_metrics()\n",
    "\n",
    "            batch_id = 0\n",
    "            for batch_features, labels in tqdm(self.data_loader.train_loader):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output, output_aggregated, total_entropy,\\\n",
    "                    aggregated_mask_values_all, mask_values_all = self.network(batch_features, training=True)\n",
    "\n",
    "                    loss = self.log_loss(labels, output)\n",
    "                    # reg_loss = sum(self.network.losses)\n",
    "                    reg_loss = cfg.train.weight_decay * tf.add_n([tf.nn.l2_loss(w) for w in self.network.trainable_variables])\n",
    "                    total_loss = loss + cfg.train.sparsity_loss_weight * total_entropy + reg_loss\n",
    "\n",
    "                grads = tape.gradient(total_loss, self.network.trainable_variables)\n",
    "                capped_gvs = [tf.clip_by_value(grad, -cfg.train.gradient_thresh,\n",
    "                                                cfg.train.gradient_thresh) for grad in grads]\n",
    "\n",
    "                self.optimizer.apply_gradients(zip(capped_gvs, self.network.trainable_variables))\n",
    "\n",
    "                self.loss_metric(total_loss)\n",
    "                self.AUC_metric(labels, output)\n",
    "                self.binary_metric(labels, output)\n",
    "\n",
    "                with self.train_summary_writer.as_default():\n",
    "                    # Visualization of the feature selection mask at decision step ni\n",
    "                    for ni in range(len(mask_values_all)):\n",
    "                        tf.summary.image(\n",
    "                            \"Mask for step\" + str(ni),\n",
    "                            tf.expand_dims(tf.expand_dims(mask_values_all[ni], 0), 3),\n",
    "                            max_outputs=1, step=step)\n",
    "                    # Visualization of the aggregated feature importances\n",
    "                    for ni in range(len(aggregated_mask_values_all)):\n",
    "                        tf.summary.image(\n",
    "                            \"Aggregated mask\",\n",
    "                            tf.expand_dims(tf.expand_dims(aggregated_mask_values_all[ni], 0), 3),\n",
    "                            max_outputs=1, step=step)\n",
    "\n",
    "                    tf.summary.scalar('Total_loss', self.loss_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Total entropy', total_entropy, step=step)\n",
    "                    tf.summary.scalar('Log_Loss', loss, step=step)\n",
    "                    tf.summary.scalar('Reg_Loss', reg_loss, step=step)\n",
    "                    tf.summary.scalar('AUC_score', self.AUC_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Accuracy', self.binary_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Gini_index', 2 * self.AUC_metric.result() - 1, step=step)\n",
    "                    tf.summary.scalar('Learning_rate', self.optimizer.learning_rate.__call__(step).numpy(), step=step)\n",
    "\n",
    "                template = 'Epoch [{}][{}/{}], loss: {}, binary accuracy: {}, AUC: {}'\n",
    "                print(template.format(epoch, batch_id, self.data_loader.nrof_batches,\n",
    "                                      loss, self.binary_metric.result() * 100,\n",
    "                                      self.AUC_metric.result() * 100))\n",
    "\n",
    "                batch_id += 1\n",
    "                step += 1\n",
    "            \n",
    "            self.__save_model(step)\n",
    "            \n",
    "            self.validation.run_validation(step)\n",
    "            \n",
    "            self.data_loader.shuffle_data()\n",
    "        \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data loading ...\n"
     ]
    }
   ],
   "source": [
    "# top25 with co feats by Gini only 4 DS\n",
    "inflow = Inflow()\n",
    "inflow.run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
